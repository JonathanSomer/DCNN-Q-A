#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
DCNN Q&A
\end_layout

\begin_layout Author
Kessler, Somer, Itzhak
\end_layout

\begin_layout Subsection*
Unanswered Questions:
\end_layout

\begin_layout Subsubsection*
When to use StandardScaling vs Normalization?
\end_layout

\begin_layout Subsubsection*
SOLVE HW3?
\end_layout

\begin_layout Section*
Week 7
\end_layout

\begin_layout Subsubsection*
Define the Sigmoid Function, state its value range:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{1}{1+e^{-x}}$
\end_inset

, value from [0,1]
\end_layout

\begin_layout Subsubsection*
State the 3 problems of the Sigmoid function:
\end_layout

\begin_layout Enumerate
At high value of input 
\begin_inset Formula $|x|$
\end_inset

 the gradient approaches 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Enumerate
Outputs only positive values (leads to the 
\begin_inset Quotes eld
\end_inset

synchronized w gradients
\begin_inset Quotes erd
\end_inset

 zig zag issue)
\end_layout

\begin_layout Enumerate
\begin_inset Formula $e^{x}$
\end_inset

 is compute expensive
\end_layout

\begin_layout Subsubsection*
Take the derivative of 
\begin_inset Formula $\sigma(x)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $\sigma(x)\left(1-\sigma(x)\right)$
\end_inset


\end_layout

\begin_layout Subsubsection*
Define the tanh Function, state its value range:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$
\end_inset

, value from [-1,1]
\end_layout

\begin_layout Subsubsection*
State an 1 advantage and 2 disadvantages of Tanh:
\end_layout

\begin_layout Standard
Advantages:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\left(-1,1\right)$
\end_inset

 range is zero centered.
\end_layout

\begin_layout Standard
Disadvantages:
\end_layout

\begin_layout Enumerate
At high value of input 
\begin_inset Formula $|x|$
\end_inset

 the gradient approaches 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $e^{x}$
\end_inset

 is compute expensive.
\end_layout

\begin_layout Subsubsection*
Define the ReLU Function
\end_layout

\begin_layout Standard
\begin_inset Formula $max(0,x)$
\end_inset

, negative are 0, and otherwise 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
State 4 advantages of Relu:
\end_layout

\begin_layout Enumerate
Doesn't saturate in + region
\end_layout

\begin_layout Enumerate
Computationally efficient
\end_layout

\begin_layout Enumerate
Converges faster than sigmoid and tanh
\end_layout

\begin_layout Enumerate
More biologically plausible than sigmoid and tanh
\end_layout

\begin_layout Subsubsection*
State 2 dis-advantages of Relu:
\end_layout

\begin_layout Enumerate
Not zero centerd
\end_layout

\begin_layout Enumerate
\begin_inset Quotes eld
\end_inset

Dead Relu
\begin_inset Quotes erd
\end_inset

 (zero gradient for negative input)
\end_layout

\begin_layout Subsubsection*
State a solution and best practice for the Dead ReLu Problem:
\end_layout

\begin_layout Standard
Initialize ReLu units with slightly positive Bias (0.01)
\end_layout

\begin_layout Subsubsection*
Define the Leaky ReLU Function
\end_layout

\begin_layout Standard
\begin_inset Formula $max(0.01x,x)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
State 4 advantages of Leaky Relu:
\end_layout

\begin_layout Enumerate
Doesn't saturate at all
\end_layout

\begin_layout Enumerate
Computationally efficient
\end_layout

\begin_layout Enumerate
Converges faster than sigmoid and tanh
\end_layout

\begin_layout Enumerate
Does not die (the dead ReLU problem)
\end_layout

\begin_layout Subsubsection*
Define the Paramatric ReLU Function (PReLU)
\end_layout

\begin_layout Standard
\begin_inset Formula $max(\alpha x,x)$
\end_inset

 where 
\begin_inset Formula $\alpha\in[0,1)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Define the Maxout Function
\end_layout

\begin_layout Standard
\begin_inset Formula $max(w_{1}\cdot x+b_{1},w_{2}\cdot x+b_{2})$
\end_inset

, just check 2 different models
\end_layout

\begin_layout Subsubsection*
Define the ELU Function
\end_layout

\begin_layout Standard
\begin_inset Formula $ELU(x)=\begin{cases}
x & x\geq0\\
\alpha(e^{x}-1) & x<0
\end{cases}$
\end_inset


\end_layout

\begin_layout Subsubsection*
What are the 5 advantages of ELU?
\end_layout

\begin_layout Enumerate
Doesn't saturate in + region
\end_layout

\begin_layout Enumerate
Converges faster than sigmoid and tanh
\end_layout

\begin_layout Enumerate
More biologically plausible than sigmoid and tanh
\end_layout

\begin_layout Enumerate
Closer to zero mean (compared with ReLU)
\end_layout

\begin_layout Enumerate
Some robustness to noise due to negative value gradient saturation.
\end_layout

\begin_layout Subsubsection*
What are the dis-advantages of ELU?
\end_layout

\begin_layout Standard
Computationally expensive
\end_layout

\begin_layout Subsubsection*
State 2 advantages of Maxout:
\end_layout

\begin_layout Enumerate
Generalizes ReLU/Leaky ReLU
\end_layout

\begin_layout Enumerate
Linear! No death.
\end_layout

\begin_layout Subsubsection*
State a disadvantage of Maxout:
\end_layout

\begin_layout Standard
Double number of paramaters.
\end_layout

\begin_layout Subsubsection*
Best practice for activation functions:
\end_layout

\begin_layout Standard
ReLU (low learning rate)-> Leaky/ELU/Maxout -> Tanh (never sigmoid)
\end_layout

\begin_layout Subsection*
Data PreProcessing
\end_layout

\begin_layout Subsubsection*
What is the typical method for preprocessing data?
\end_layout

\begin_layout Standard
Subtract column means, divide by column std.
\end_layout

\begin_layout Subsubsection*
Which 2 methods for preprocessing images are commonly used?
\end_layout

\begin_layout Standard
Center only (now division by STD):
\end_layout

\begin_layout Enumerate
Remove mean image (per pixel mean 
\begin_inset Formula $H\times W\times C$
\end_inset

)
\end_layout

\begin_layout Enumerate
Remove mean per channel (per channel mean 
\begin_inset Formula $C$
\end_inset

)
\end_layout

\begin_layout Subsection*
Weight Initialization:
\end_layout

\begin_layout Subsubsection*
What is the problem with constant weight initialization?
\end_layout

\begin_layout Standard
All nodes treated symetrically.
\end_layout

\begin_layout Subsubsection*
What is the formula for 
\begin_inset Formula $VAR(X\cdot Y)$
\end_inset

 for independent 
\begin_inset Formula $X,Y$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $\sigma_{X}^{2}\sigma_{Y}^{2}+\sigma_{X}^{2}\mu_{Y}^{2}+\sigma_{Y}^{2}\mu_{X}^{2}$
\end_inset


\end_layout

\begin_layout Subsubsection*
10 layers, 500 neurons per each, tanh activation.
 What is the problem with initializing all layers with a 
\begin_inset Formula $0.01$
\end_inset

 standard deviation, 
\begin_inset Formula $0$
\end_inset

 mean normal distribution? (describe the corresponding experiment)
\end_layout

\begin_layout Standard
All layer mean activations go to 0, all layers std go to 0.
 The reason is that we arrive at a 
\begin_inset Formula $0$
\end_inset

 mean distribution with very low STD.
\end_layout

\begin_layout Subsubsection*
10 layers, 500 neurons per each, tanh activation.
 What is the problem with initializing all layers with a 
\begin_inset Formula $1$
\end_inset

 standard deviation, 
\begin_inset Formula $0$
\end_inset

 mean normal distribution? (describe the corresponding experiment)
\end_layout

\begin_layout Standard
All neurons are activated at 
\begin_inset Formula $\{-1,1\}$
\end_inset

 (saturated) due to large variance in the input (500ish).
\end_layout

\begin_layout Subsubsection*
Describe the 
\begin_inset Quotes eld
\end_inset

Xavier Initialization
\begin_inset Quotes erd
\end_inset

 method:
\end_layout

\begin_layout Standard
\begin_inset Formula $W=np.randn(\text{fan\_in, fan\_out})\cdot\frac{1}{\sqrt{\text{fan\_in}}}$
\end_inset

, justification - 
\begin_inset Formula $\sum_{i=1}^{\text{fan in}}w_{i}x_{i}\sim N(0,1)$
\end_inset


\end_layout

\begin_layout Subsubsection*
Why does this fail with ReLU, and in what way?
\end_layout

\begin_layout Standard
We divide by 
\begin_inset Formula $\sqrt{\text{fan in}},$
\end_inset

but due to ReLU half of the weights are 0.
 So we should divide by 
\begin_inset Formula $\frac{\sqrt{\text{fan in}}}{2}$
\end_inset


\end_layout

\begin_layout Subsubsection*
How is the above initialization called?
\end_layout

\begin_layout Standard
He
\end_layout

\begin_layout Subsection*
Batch Normalization
\end_layout

\begin_layout Subsubsection*
Describe the Batch Normalization method, before which layers do we usually
 place a BN layers?:
\end_layout

\begin_layout Standard
During training: Before the activation function, usually after FC or Convolution
al layer:
\end_layout

\begin_layout Enumerate
Compute batch mean (at layer) 
\begin_inset Formula $\mu_{B}$
\end_inset


\end_layout

\begin_layout Enumerate
Compute batch std (at layer) 
\begin_inset Formula $\sigma_{B}$
\end_inset


\end_layout

\begin_layout Enumerate
Per sample activation 
\begin_inset Formula $x_{i}$
\end_inset

 compute a normalized version of the sample's activation: 
\begin_inset Formula $\hat{x_{i}}=\frac{x_{i}-\mu_{B}}{\sigma+\epsilon}$
\end_inset


\end_layout

\begin_layout Enumerate
Scale: 
\begin_inset Formula $\hat{x_{i}}\cdot\gamma+\beta\equiv BN_{\gamma,\beta}(x_{i})$
\end_inset


\end_layout

\begin_layout Subsubsection*
4 Advantages of BN?
\end_layout

\begin_layout Enumerate
The gradient flow improve (because of the gradient's scale)
\end_layout

\begin_layout Enumerate
Allow higher learning rates (because of the gradient's scale)
\end_layout

\begin_layout Enumerate
Reduce dependency on initialization
\end_layout

\begin_layout Enumerate
Regularization: each batch is normalized reletive to itself, harder to overfit
\end_layout

\begin_layout Subsubsection*
Describe BN at test time:
\end_layout

\begin_layout Standard
Instead of normalizing to the batch, normalize reletive to an estimated
 mean and varance of the input data (which is computed during the training
 time)
\end_layout

\begin_layout Subsection*
Babysitting the learning process:
\end_layout

\begin_layout Subsubsection*
Describe the generic processes of creating and training a neural net:
\end_layout

\begin_layout Enumerate
Pre-process data
\end_layout

\begin_layout Enumerate
Choose architecture
\end_layout

\begin_layout Enumerate
See reasonable loss + loss increase after adding regularization
\end_layout

\begin_layout Enumerate
Tweak learning rate:
\end_layout

\begin_deeper
\begin_layout Enumerate
No improvement = too small
\end_layout

\begin_layout Enumerate
Explodes = too big
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection*
Hyperparamater Tuning:
\end_layout

\begin_layout Subsubsection*
What is the general principle for searching for optimal paramaters?
\end_layout

\begin_layout Standard
Start with a coarse search (find correct order of magnitude), then a more
 fine search.
\end_layout

\begin_layout Subsubsection*
Name two methods for hyperparamter searching
\end_layout

\begin_layout Standard
Grid Search, Random Search
\end_layout

\begin_layout Subsubsection*
In what setting is the Random Search particularly effective?
\end_layout

\begin_layout Standard
One important paramater, one non-important.
 Instead of checking a small number of values of the important paramater
 against all values of the non-important, we get a random spread of values
 of the important one.
\end_layout

\begin_layout Subsubsection*
Give examples of hyperparamaters:
\end_layout

\begin_layout Enumerate
Architecture
\end_layout

\begin_layout Enumerate
Learning rate + update algos etc.
\end_layout

\begin_layout Enumerate
Regularization
\end_layout

\begin_layout Subsubsection*
What does the following image indicate?
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Bad initialization
\end_layout

\begin_layout Subsubsection*
You see a large gap between validation and train error, what you do?
\end_layout

\begin_layout Standard
Overfitting! Increase regularization.
\end_layout

\begin_layout Subsubsection*
You see no gap between validation and train error, what you do?
\end_layout

\begin_layout Standard
Increase model capacity?
\end_layout

\begin_layout Subsubsection*
What will we never think of, as a method for validating correct learning
 rate?
\end_layout

\begin_layout Standard
See that weight updates are around 0.01 of the weight magnitude.
 (norm of current weights, norm of entire update vector)
\end_layout

\begin_layout Section*
Week 8a
\end_layout

\end_body
\end_document
