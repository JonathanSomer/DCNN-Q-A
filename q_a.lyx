#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{algorithm,algpseudocode}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
DCNN Q&A
\end_layout

\begin_layout Author
Kessler, Somer, Itzhak
\end_layout

\begin_layout Subsection*
Unanswered Questions:
\end_layout

\begin_layout Subsubsection*
When to use StandardScaling vs Normalization?
\end_layout

\begin_layout Subsubsection*
SOLVE HW3?
\end_layout

\begin_layout Subsubsection*
Instance Normalization - don't we lose lots of information from normalzing
 each channel - 
\begin_inset Quotes eld
\end_inset

the fire channel
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Section*
Week 7
\end_layout

\begin_layout Subsubsection*
Define the Sigmoid Function, state its value range:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{1}{1+e^{-x}}$
\end_inset

, value from [0,1]
\end_layout

\begin_layout Subsubsection*
State the 3 problems of the Sigmoid function:
\end_layout

\begin_layout Enumerate
At high value of input 
\begin_inset Formula $|x|$
\end_inset

 the gradient approaches 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Enumerate
Outputs only positive values (leads to the 
\begin_inset Quotes eld
\end_inset

synchronized w gradients
\begin_inset Quotes erd
\end_inset

 zig zag issue)
\end_layout

\begin_layout Enumerate
\begin_inset Formula $e^{x}$
\end_inset

 is compute expensive
\end_layout

\begin_layout Subsubsection*
Take the derivative of 
\begin_inset Formula $\sigma(x)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $\sigma(x)\left(1-\sigma(x)\right)$
\end_inset


\end_layout

\begin_layout Subsubsection*
Define the tanh Function, state its value range:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$
\end_inset

, value from [-1,1]
\end_layout

\begin_layout Subsubsection*
State an 1 advantage and 2 disadvantages of Tanh:
\end_layout

\begin_layout Standard
Advantages:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\left(-1,1\right)$
\end_inset

 range is zero centered.
\end_layout

\begin_layout Standard
Disadvantages:
\end_layout

\begin_layout Enumerate
At high value of input 
\begin_inset Formula $|x|$
\end_inset

 the gradient approaches 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $e^{x}$
\end_inset

 is compute expensive.
\end_layout

\begin_layout Subsubsection*
Define the ReLU Function
\end_layout

\begin_layout Standard
\begin_inset Formula $max(0,x)$
\end_inset

, negative are 0, and otherwise 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
State 4 advantages of Relu:
\end_layout

\begin_layout Enumerate
Doesn't saturate in + region
\end_layout

\begin_layout Enumerate
Computationally efficient
\end_layout

\begin_layout Enumerate
Converges faster than sigmoid and tanh
\end_layout

\begin_layout Enumerate
More biologically plausible than sigmoid and tanh
\end_layout

\begin_layout Subsubsection*
State 2 dis-advantages of Relu:
\end_layout

\begin_layout Enumerate
Not zero centerd
\end_layout

\begin_layout Enumerate
\begin_inset Quotes eld
\end_inset

Dead Relu
\begin_inset Quotes erd
\end_inset

 (zero gradient for negative input)
\end_layout

\begin_layout Subsubsection*
State a solution and best practice for the Dead ReLu Problem:
\end_layout

\begin_layout Standard
Initialize ReLu units with slightly positive Bias (0.01)
\end_layout

\begin_layout Subsubsection*
Define the Leaky ReLU Function
\end_layout

\begin_layout Standard
\begin_inset Formula $max(0.01x,x)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
State 4 advantages of Leaky Relu:
\end_layout

\begin_layout Enumerate
Doesn't saturate at all
\end_layout

\begin_layout Enumerate
Computationally efficient
\end_layout

\begin_layout Enumerate
Converges faster than sigmoid and tanh
\end_layout

\begin_layout Enumerate
Does not die (the dead ReLU problem)
\end_layout

\begin_layout Subsubsection*
Define the Paramatric ReLU Function (PReLU)
\end_layout

\begin_layout Standard
\begin_inset Formula $max(\alpha x,x)$
\end_inset

 where 
\begin_inset Formula $\alpha\in[0,1)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Define the Maxout Function
\end_layout

\begin_layout Standard
\begin_inset Formula $max(w_{1}\cdot x+b_{1},w_{2}\cdot x+b_{2})$
\end_inset

, just check 2 different models
\end_layout

\begin_layout Subsubsection*
Define the ELU Function
\end_layout

\begin_layout Standard
\begin_inset Formula $ELU(x)=\begin{cases}
x & x\geq0\\
\alpha(e^{x}-1) & x<0
\end{cases}$
\end_inset


\end_layout

\begin_layout Subsubsection*
What are the 5 advantages of ELU?
\end_layout

\begin_layout Enumerate
Doesn't saturate in + region
\end_layout

\begin_layout Enumerate
Converges faster than sigmoid and tanh
\end_layout

\begin_layout Enumerate
More biologically plausible than sigmoid and tanh
\end_layout

\begin_layout Enumerate
Closer to zero mean (compared with ReLU)
\end_layout

\begin_layout Enumerate
Some robustness to noise due to negative value gradient saturation.
\end_layout

\begin_layout Subsubsection*
What are the dis-advantages of ELU?
\end_layout

\begin_layout Standard
Computationally expensive
\end_layout

\begin_layout Subsubsection*
State 2 advantages of Maxout:
\end_layout

\begin_layout Enumerate
Generalizes ReLU/Leaky ReLU
\end_layout

\begin_layout Enumerate
Linear! No death.
\end_layout

\begin_layout Subsubsection*
State a disadvantage of Maxout:
\end_layout

\begin_layout Standard
Double number of paramaters.
\end_layout

\begin_layout Subsubsection*
Best practice for activation functions:
\end_layout

\begin_layout Standard
ReLU (low learning rate)-> Leaky/ELU/Maxout -> Tanh (never sigmoid)
\end_layout

\begin_layout Subsection*
Data PreProcessing
\end_layout

\begin_layout Subsubsection*
What is the typical method for preprocessing data?
\end_layout

\begin_layout Standard
Subtract column means, divide by column std.
\end_layout

\begin_layout Subsubsection*
Which 2 methods for preprocessing images are commonly used?
\end_layout

\begin_layout Standard
Center only (now division by STD):
\end_layout

\begin_layout Enumerate
Remove mean image (per pixel mean 
\begin_inset Formula $H\times W\times C$
\end_inset

)
\end_layout

\begin_layout Enumerate
Remove mean per channel (per channel mean 
\begin_inset Formula $C$
\end_inset

)
\end_layout

\begin_layout Subsection*
Weight Initialization:
\end_layout

\begin_layout Subsubsection*
What is the problem with constant weight initialization?
\end_layout

\begin_layout Standard
All nodes treated symetrically.
\end_layout

\begin_layout Subsubsection*
What is the formula for 
\begin_inset Formula $VAR(X\cdot Y)$
\end_inset

 for independent 
\begin_inset Formula $X,Y$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $\sigma_{X}^{2}\sigma_{Y}^{2}+\sigma_{X}^{2}\mu_{Y}^{2}+\sigma_{Y}^{2}\mu_{X}^{2}$
\end_inset


\end_layout

\begin_layout Subsubsection*
10 layers, 500 neurons per each, tanh activation.
 What is the problem with initializing all layers with a 
\begin_inset Formula $0.01$
\end_inset

 standard deviation, 
\begin_inset Formula $0$
\end_inset

 mean normal distribution? (describe the corresponding experiment)
\end_layout

\begin_layout Standard
All layer mean activations go to 0, all layers std go to 0.
 The reason is that we arrive at a 
\begin_inset Formula $0$
\end_inset

 mean distribution with very low STD.
\end_layout

\begin_layout Subsubsection*
10 layers, 500 neurons per each, tanh activation.
 What is the problem with initializing all layers with a 
\begin_inset Formula $1$
\end_inset

 standard deviation, 
\begin_inset Formula $0$
\end_inset

 mean normal distribution? (describe the corresponding experiment)
\end_layout

\begin_layout Standard
All neurons are activated at 
\begin_inset Formula $\{-1,1\}$
\end_inset

 (saturated) due to large variance in the input (500ish).
\end_layout

\begin_layout Subsubsection*
Describe the 
\begin_inset Quotes eld
\end_inset

Xavier Initialization
\begin_inset Quotes erd
\end_inset

 method:
\end_layout

\begin_layout Standard
\begin_inset Formula $W=np.randn(\text{fan\_in, fan\_out})\cdot\frac{1}{\sqrt{\text{fan\_in}}}$
\end_inset

, justification - 
\begin_inset Formula $\sum_{i=1}^{\text{fan in}}w_{i}x_{i}\sim N(0,1)$
\end_inset


\end_layout

\begin_layout Subsubsection*
Why does this fail with ReLU, and in what way?
\end_layout

\begin_layout Standard
We divide by 
\begin_inset Formula $\sqrt{\text{fan in}},$
\end_inset

but due to ReLU half of the weights are 0.
 So we should divide by 
\begin_inset Formula $\frac{\sqrt{\text{fan in}}}{2}$
\end_inset


\end_layout

\begin_layout Subsubsection*
How is the above initialization called?
\end_layout

\begin_layout Standard
He
\end_layout

\begin_layout Subsection*
Batch Normalization
\end_layout

\begin_layout Subsubsection*
Describe the Batch Normalization method, before which layers do we usually
 place a BN layers?:
\end_layout

\begin_layout Standard
During training: Before the activation function, usually after FC or Convolution
al layer:
\end_layout

\begin_layout Enumerate
Compute batch mean (at layer) 
\begin_inset Formula $\mu_{B}$
\end_inset


\end_layout

\begin_layout Enumerate
Compute batch std (at layer) 
\begin_inset Formula $\sigma_{B}$
\end_inset


\end_layout

\begin_layout Enumerate
Per sample activation 
\begin_inset Formula $x_{i}$
\end_inset

 compute a normalized version of the sample's activation: 
\begin_inset Formula $\hat{x_{i}}=\frac{x_{i}-\mu_{B}}{\sigma+\epsilon}$
\end_inset


\end_layout

\begin_layout Enumerate
Scale: 
\begin_inset Formula $\hat{x_{i}}\cdot\gamma+\beta\equiv BN_{\gamma,\beta}(x_{i})$
\end_inset


\end_layout

\begin_layout Subsubsection*
4 Advantages of BN?
\end_layout

\begin_layout Enumerate
The gradient flow improve (because of the gradient's scale)
\end_layout

\begin_layout Enumerate
Allow higher learning rates (because of the gradient's scale)
\end_layout

\begin_layout Enumerate
Reduce dependency on initialization
\end_layout

\begin_layout Enumerate
Regularization: each batch is normalized reletive to itself, harder to overfit
\end_layout

\begin_layout Subsubsection*
Describe BN at test time:
\end_layout

\begin_layout Standard
Instead of normalizing to the batch, normalize reletive to an estimated
 mean and varance of the input data (which is computed during the training
 time)
\end_layout

\begin_layout Subsection*
Babysitting the learning process:
\end_layout

\begin_layout Subsubsection*
Describe the generic processes of creating and training a neural net:
\end_layout

\begin_layout Enumerate
Pre-process data
\end_layout

\begin_layout Enumerate
Choose architecture
\end_layout

\begin_layout Enumerate
See reasonable loss + loss increase after adding regularization
\end_layout

\begin_layout Enumerate
Tweak learning rate:
\end_layout

\begin_deeper
\begin_layout Enumerate
No improvement = too small
\end_layout

\begin_layout Enumerate
Explodes = too big
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection*
Hyperparamater Tuning:
\end_layout

\begin_layout Subsubsection*
What is the general principle for searching for optimal paramaters?
\end_layout

\begin_layout Standard
Start with a coarse search (find correct order of magnitude), then a more
 fine search.
\end_layout

\begin_layout Subsubsection*
Name two methods for hyperparamter searching
\end_layout

\begin_layout Standard
Grid Search, Random Search
\end_layout

\begin_layout Subsubsection*
In what setting is the Random Search particularly effective?
\end_layout

\begin_layout Standard
One important paramater, one non-important.
 Instead of checking a small number of values of the important paramater
 against all values of the non-important, we get a random spread of values
 of the important one.
\end_layout

\begin_layout Subsubsection*
Give examples of hyperparamaters:
\end_layout

\begin_layout Enumerate
Architecture
\end_layout

\begin_layout Enumerate
Learning rate + update algos etc.
\end_layout

\begin_layout Enumerate
Regularization
\end_layout

\begin_layout Subsubsection*
What does the following image indicate?
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png
	width 11cm

\end_inset


\end_layout

\begin_layout Standard
Bad initialization
\end_layout

\begin_layout Subsubsection*
You see a large gap between validation and train error, what you do?
\end_layout

\begin_layout Standard
Overfitting! Increase regularization.
\end_layout

\begin_layout Subsubsection*
You see no gap between validation and train error, what you do?
\end_layout

\begin_layout Standard
Increase model capacity?
\end_layout

\begin_layout Subsubsection*
What will we never think of, as a method for validating correct learning
 rate?
\end_layout

\begin_layout Standard
See that weight updates are around 0.01 of the weight magnitude.
 (norm of current weights, norm of entire update vector)
\end_layout

\begin_layout Section*
Week 8a
\end_layout

\begin_layout Subsubsection*
How do we batch normalize images?
\end_layout

\begin_layout Standard
Per channel.
\end_layout

\begin_layout Subsubsection*
What is Layer Normalization?
\end_layout

\begin_layout Standard
Normalize each example independently using all of its features: 
\begin_inset Formula 
\begin{align*}
\gamma,\beta\in1\times D\\
\mu,\sigma\in1\times N\\
y=\gamma(x-\mu)/\sigma+\beta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Same in train and test.
\end_layout

\begin_layout Subsubsection*
When is Layer Normalization needed, why?
\end_layout

\begin_layout Standard
RNNs.
 Learning constant 
\begin_inset Formula $\gamma,\beta$
\end_inset

 can lead to bad behavior in RNNs (explode/implode).
\end_layout

\begin_layout Subsubsection*
What is Instance Normalization?
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

Layer Normalization
\begin_inset Quotes erd
\end_inset

 for images.
 Normalize each example independently, per channel using all pixels of that
 channel: 
\begin_inset Formula 
\begin{align*}
\gamma,\beta\in1\times C\\
\mu,\sigma\in N\times C\\
y=\gamma(x-\mu)/\sigma+\beta
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Same in train and test.
\end_layout

\begin_layout Subsubsection*
Draw the 3 cubes of Normalization for images:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted2.png
	width 13cm

\end_inset


\end_layout

\begin_layout Subsubsection*
What is group norm?
\end_layout

\begin_layout Standard
Same as instance norm except we use a group of channels (arbitrarily chosen?)
\end_layout

\begin_layout Subsubsection*
What is DBN?
\end_layout

\begin_layout Standard
Decorrelated Batch Norm.
 Take the decorrelated 
\begin_inset Formula $x$
\end_inset

, remove mean, divide each feature by it's standard deviation.
 Now we have 
\begin_inset Quotes eld
\end_inset

whitened data
\begin_inset Quotes erd
\end_inset


\begin_inset Formula 
\[
\Sigma^{-\frac{1}{2}}\left(x_{i}-\mu\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
State 3 problems of Vanilla SGD?
\end_layout

\begin_layout Enumerate
There can be large changes in one direction relative to others so learning
 rate does not fit all directions equally well.
\end_layout

\begin_layout Enumerate
Presence of local minima or saddle points
\end_layout

\begin_layout Enumerate
Gradients are noisy due to use of mini-batches.
\end_layout

\begin_layout Subsubsection*
What happens in SGD if in one direction we have a large gradient and in
 another a small gradient?
\end_layout

\begin_layout Standard
Jumps in large direction, slow in small direction.
\end_layout

\begin_layout Subsubsection*
What is the condition number of a hessian matrix? <LEARN MORE ABOUT THIS?>
\end_layout

\begin_layout Standard
Ratio of largest to smallest singular value.
\end_layout

\begin_layout Subsubsection*
Write down the vanilla sgd update rule of the weights?
\end_layout

\begin_layout Standard
\begin_inset Formula $W_{t+1}=W_{t}-\alpha\nabla f(W_{t})$
\end_inset


\end_layout

\begin_layout Subsubsection*
State a SGD update rule which enables the algorithm to overcome saddle points
 and local minima (zero gradients)?
\end_layout

\begin_layout Standard
Momentum!
\begin_inset Formula 
\begin{align*}
v_{0} & =0\\
v_{t+1} & =\rho v_{t}+\nabla f(W_{t})\\
W_{t+1} & =W_{t}-\alpha v_{t+1}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection*
Give an alternative formulation to the momentum formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
v_{0} & =0\\
v_{t+1} & =\rho v_{t}-\alpha\nabla f(W_{t})\\
W_{t+1} & =W_{t}+v_{t+1}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection*
In the Momentum method, what is the problem with adding the gradient at
 
\begin_inset Formula $W_{t}$
\end_inset

?
\end_layout

\begin_layout Standard
We are moving, the gradient at 
\begin_inset Formula $W_{t}$
\end_inset

 might not be relevant if we will move far from it due to 
\begin_inset Formula $v_{t}$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
What is the solution to the previous issue (develops second formulation
 of Momentum formula)?
\end_layout

\begin_layout Standard
Nesterov momentum.
\begin_inset Formula 
\begin{align*}
v_{0} & =0\\
v_{t+1} & =\rho v_{t}-\alpha\nabla f(W_{t}+\rho v_{t})\\
W_{t+1} & =W_{t}+v_{t+1}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection*
What is a common re-paramatization of the Nesterov rule?
\end_layout

\begin_layout Standard
Denote:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\tilde{W}_{t}=W_{t}+\rho v_{t}
\]

\end_inset

 The update rule becomes:
\begin_inset Formula 
\[
\tilde{W}_{t+1}=\tilde{W}_{t}+v_{t+1}+\rho(v_{t+1}-v_{t})
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
How do we overcome the issue of different sized gradients in different direction
s?
\end_layout

\begin_layout Standard
AdaGrad! We sum the square of the gradients at each step, and divide each
 gradient by the square root of this sum.
 (the norm of the vector of all gradients seen thus far for this direction)
\begin_inset Newline newline
\end_inset

Algorithm:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted3.png
	width 13cm

\end_inset


\end_layout

\begin_layout Subsubsection*
What happens for directions with large gradient?
\end_layout

\begin_layout Standard
Smaller steps.
\end_layout

\begin_layout Subsubsection*
What happens for directions with small gradient?
\end_layout

\begin_layout Standard
Large steps initially.
\end_layout

\begin_layout Subsubsection*
What happens to the step size over long time?
\end_layout

\begin_layout Standard
Goes to zero.
\end_layout

\begin_layout Subsubsection*
How do you prevent this from happening?
\end_layout

\begin_layout Standard
RMSprop - apply a decay to the sum of square gradients from AdaGrad (typically
 0.9).
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted4.png
	width 13cm

\end_inset


\end_layout

\begin_layout Subsubsection*
Why is this decay computation not exactly a weighted average?
\end_layout

\begin_layout Standard
Initial gradient is 
\begin_inset Formula $0$
\end_inset

!
\end_layout

\begin_layout Subsubsection*
How is the combination of RMSprop and Momentum called?
\end_layout

\begin_layout Standard
Adam!
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted5.png
	width 13cm

\end_inset


\end_layout

\begin_layout Subsubsection*
Explain the unbiasing term:
\end_layout

\begin_layout Standard
After unfolding we have the following weighted average (assuming first moment
 is initialized to 0):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{m_{t}}=\frac{\beta^{t-1}g_{1}+\beta^{t-2}g_{2}+....g_{t}}{\beta^{t-1}+\beta^{t-2}+...+1}
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
What hyperparamater do all of these methods have?
\end_layout

\begin_layout Standard
Learning Rate
\end_layout

\begin_layout Subsubsection*
Draw a graph with 4 cases for constant learning rate:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted6.png

\end_inset


\end_layout

\begin_layout Subsubsection*
State 3 Methods for learning rate variation:
\end_layout

\begin_layout Enumerate
Step: divide by half every 
\begin_inset Formula $n$
\end_inset

 epochs.
\end_layout

\begin_layout Enumerate
Exponential decay: 
\begin_inset Formula $\alpha_{t}=\alpha_{0}e^{-kt}$
\end_inset


\end_layout

\begin_layout Enumerate
1/t decay: 
\begin_inset Formula $\alpha_{t}=\frac{\alpha_{0}}{\left(1+kt\right)}$
\end_inset


\end_layout

\begin_layout Subsubsection*
Why is learning rate decay less common with Adam than with SGD + Momentum:
\end_layout

\begin_layout Standard
Adam has the learning rate adjusted per paramater according to size of the
 gradient.
\end_layout

\begin_layout Subsubsection*
What is the difference in the step taken using second order optimization?
\end_layout

\begin_layout Standard
In first order we take a small step in the direction of decrease.
 In second order optimization we take a guess at the minimum directly.
\end_layout

\begin_layout Subsubsection*
Write down the second order multi-variate taylor series:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J\left(\theta\right)\approx J\left(\theta_{0}\right)+\left(\theta-\theta_{0}\right)^{T}\nabla J\left(\theta_{0}\right)+\frac{1}{2}\left(\theta-\theta_{0}\right)^{T}{\bf H}\left(\theta-\theta_{0}\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
\begin_inset Formula $\frac{\partial x^{T}Ax}{\partial x}$
\end_inset

=?
\end_layout

\begin_layout Standard
\begin_inset Formula $\left(A+A^{T}\right)x$
\end_inset


\end_layout

\begin_layout Subsubsection*
What is the critical point, according to the above expansion:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial J\left(\theta\right)}{\partial\theta} & =\nabla J\left(\theta_{0}\right)+{\bf H}\left(\theta-\theta_{0}\right)=0\\
{\bf H}\left(\theta-\theta_{0}\right) & =-\nabla J\left(\theta_{0}\right)\\
\theta & =\theta_{0}-{\bf H}^{-1}\nabla J\left(\theta_{0}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection*
How many hyperparamaters does a second order optimization scheme have?
\end_layout

\begin_layout Standard
None!
\end_layout

\begin_layout Subsubsection*
Why can't we use this in practice?
\end_layout

\begin_layout Standard
Complexity of computing 
\begin_inset Formula ${\bf H}^{-1}$
\end_inset

 is 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
What is the name of an algorithm for Solving this in 
\begin_inset Formula $O\left(n^{2}\right)$
\end_inset

 time?
\end_layout

\begin_layout Standard
BFGS or L-BFGS
\end_layout

\begin_layout Subsubsection*
In what setting is L-BFGS useful?
\end_layout

\begin_layout Standard
Full batch, deterministic (consistent 
\begin_inset Formula $f\left(x\right)$
\end_inset

 to be optimized)
\end_layout

\begin_layout Subsubsection*
How do we know how many epochs to use?
\end_layout

\begin_layout Standard
Early stopping - stop when validation error starts increasing, or train
 for a lot of epochs and keep a snapshot of best performing model.
\end_layout

\begin_layout Subsubsection*
What are model ensembles?
\end_layout

\begin_layout Standard
A method for reducing variance via averaging of many models.
\end_layout

\begin_layout Subsubsection*
How can we use model ensembles in the Deep Learning setting without training
 many different models from scratch?
\end_layout

\begin_layout Standard
Use different snapshots of the same model.
\end_layout

\begin_layout Subsubsection*
A specific model seems to just improve its performance over time.
 How can we still use snapshots while having relatively varying models?
\end_layout

\begin_layout Standard
Cyclic Learning rate.
 Such as cosine annealing - start with 
\begin_inset Formula $\alpha=\alpha_{\min}+\frac{1}{2}\left(\alpha_{\max}-\alpha_{\min}\right)\left(1+\cos\left(\pi\frac{T\mod N+1}{N}\right)\right)$
\end_inset


\end_layout

\begin_layout Subsubsection*
How can we implement snapshot model ensembles without holding many paramater
 vectors?
\end_layout

\begin_layout Standard
Polyak Averaging.
 Compute a running average of the paramaters:
\begin_inset Formula 
\[
W_{\text{test}}=0.995W_{\text{test}}+0.005W_{t}
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
State _ methods for Regularization?
\end_layout

\begin_layout Enumerate
Add weight decay term to loss: 
\begin_inset Formula $L_{2},L_{1},\text{or a weighted mix of both}$
\end_inset

 (elastic net)
\end_layout

\begin_layout Enumerate
Dropout
\end_layout

\begin_layout Enumerate
Randomness - add randomness in training, average out in testing (see next
 questions)
\end_layout

\begin_layout Enumerate
Data Augmentation
\end_layout

\begin_layout Enumerate
DropConnect
\end_layout

\begin_layout Enumerate
Fractional Max Pooling
\end_layout

\begin_layout Enumerate
Stochastic Depth
\end_layout

\begin_layout Subsubsection*
Explain Dropout:
\end_layout

\begin_layout Standard
In the forward pass set each neuron to zero with probability 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
State two interpretations to why Dropout could be useful:
\end_layout

\begin_layout Enumerate
Force redundancy of features and prevent co-adaptation
\end_layout

\begin_layout Enumerate
Train an ensemble of models, each with slightly different architecture,
 all share some paramaters.
\end_layout

\begin_layout Subsubsection*
What is the issue with using models trained with Droput in the test phase?
\end_layout

\begin_layout Standard
Droupout makes output random:
\begin_inset Formula 
\[
y=f_{W}(\underset{\text{input image}}{x},\underset{\text{random mask}}{z})
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
What is the output we would like to approximate?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbb{E}_{z}\left[f_{W}(x,z)\right]=\int_{z}p\left(z\right)f_{W}(x,z)dz
\]

\end_inset


\end_layout

\begin_layout Standard
This is the randomness method 
\begin_inset Quotes eld
\end_inset

template from before 
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsubsection*
Why can't we simply activate all neurons in test time?
\end_layout

\begin_layout Standard
If we have trained with only 
\begin_inset Formula $p$
\end_inset

 of the neurons in each layer activated, we will have a larger expected
 value in test time.
\end_layout

\begin_layout Subsubsection*
What are 2 possible solutions for this?
\end_layout

\begin_layout Standard
In both solutions we activate all neurons in test time.
 The solutions differ in the time in which we scale the outputs.
 Assuming a neuron is kept with probability 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Enumerate
In test time, scale all activations of each layer by 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Enumerate
In training time, divide the activations of each layer by 
\begin_inset Formula $p$
\end_inset

.
 (testing becomes a simple feedforward)
\end_layout

\begin_layout Subsubsection*
How is the second method called?
\end_layout

\begin_layout Standard
Inverted Dropout.
\end_layout

\begin_layout Subsubsection*
State _ methods for data augmentation:
\end_layout

\begin_layout Enumerate
Flips
\end_layout

\begin_layout Enumerate
Random crops and scales
\end_layout

\begin_layout Enumerate
Color jittering - randomize contrast and brightness
\end_layout

\begin_layout Enumerate
Distortions - lenses, stretch, translation, shearing etc.
\end_layout

\begin_layout Subsubsection*
How does transfer learning differ when using large/small datasets (both
 relatively similar to the original one)?
\end_layout

\begin_layout Standard
Small - finetune only last layer.
 Large - maybe finetune a few layers
\end_layout

\begin_layout Subsubsection*
How does transfer learning differ when the target dataset is very different
 from the original one.
 Explain for small/large amount of data?
\end_layout

\begin_layout Standard
Lots of data - finetune lots of layers.
 Little data - try linear classifiers from different layers, probably can't
 finetune many layers on a huge net using little data.
\end_layout

\begin_layout Section*
Week 8b - CNN Architectures
\end_layout

\begin_layout Subsection*
Given image of input 
\begin_inset Formula $227\times227\times3$
\end_inset

, and Conv2d layer with 96 filters of size
\begin_inset Formula $11\times11$
\end_inset

 with stride 
\begin_inset Formula $4$
\end_inset

, what would be the output size:
\end_layout

\begin_layout Standard
The width and height is 
\begin_inset Formula $\left(227-11\right)/4+1=55$
\end_inset

, so 
\begin_inset Formula $55\times55\times96$
\end_inset


\end_layout

\begin_layout Subsection*
Given image of input 
\begin_inset Formula $227\times227\times3$
\end_inset

, and Conv2d layer with 96 filters of size
\begin_inset Formula $11\times11$
\end_inset

 with stride 
\begin_inset Formula $4$
\end_inset

, how many parameters are there:
\end_layout

\begin_layout Standard
\begin_inset Formula $11\cdot11\cdot3$
\end_inset

 for every filter, so 
\begin_inset Formula $11\cdot11\cdot3\cdot96=34848$
\end_inset


\end_layout

\begin_layout Subsection*
Given input 
\begin_inset Formula $55\times55\times96$
\end_inset

, what will be the output of a 
\begin_inset Formula $Max\,Pool$
\end_inset

 layer with 
\begin_inset Formula $3\times3$
\end_inset

 filter size and stride 
\begin_inset Formula $2$
\end_inset

:
\end_layout

\begin_layout Standard
The width and height of the new input will be: 
\begin_inset Formula $(55-3)/2+1=27$
\end_inset

, so 
\begin_inset Formula $27\times27\times96$
\end_inset


\end_layout

\begin_layout Subsection*
Given image of input 
\begin_inset Formula $13\times13\times256$
\end_inset

, and Conv2d layer with 96 filters of size 
\begin_inset Formula $3\times3$
\end_inset

 with stride 
\begin_inset Formula $1$
\end_inset

, and 
\begin_inset Formula $1$
\end_inset

 padding what would be the output size:
\end_layout

\begin_layout Standard
\begin_inset Formula $\left(\left(13+2\cdot1\right)-3\right)/1+1=13$
\end_inset

, 
\begin_inset Formula $13\times13\times96$
\end_inset


\end_layout

\begin_layout Subsection*
How is ZFNet different from AlexNet?
\end_layout

\begin_layout Standard
ZFNet use the same architecture of AlexNet, but keep higher resolution at
 the beginning(smaller stride at start), and use more channels at the deeper
 layers.
\end_layout

\begin_layout Subsection*
What are VGGNet building blocks?
\end_layout

\begin_layout Standard
VGGNet is builded with only 
\begin_inset Formula $3\times3$
\end_inset

 filter convolution layers, 
\begin_inset Formula $2\times2$
\end_inset

 max-pool stide 
\begin_inset Formula $2$
\end_inset

 layers, and at the end fully connected layers
\end_layout

\begin_layout Subsection*
What were the 2 main changes that VGG net presented reletive to AlexNet
 and ZFNet?
\end_layout

\begin_layout Enumerate
Very small fillter right from the start that does not change the images
 sizse (
\begin_inset Formula $\left(3,3\right)$
\end_inset

, stride 
\begin_inset Formula $1$
\end_inset

, padding 
\begin_inset Formula $1$
\end_inset

)
\end_layout

\begin_layout Enumerate
More layers (deeper net)
\end_layout

\begin_layout Subsection*
What are the benefits of smaller fillter in VGGNet, for example a 
\begin_inset Formula $3\times3$
\end_inset

 filter reletive to 
\begin_inset Formula $7\times7$
\end_inset

 filter?
\end_layout

\begin_layout Enumerate

\series bold
Deeper: 
\series default
You can stack 
\begin_inset Formula $3$
\end_inset

 Conv layer with filter 
\begin_inset Formula $3\times3$
\end_inset

 of each other to get the same receptive field of a 
\begin_inset Formula $7\times7$
\end_inset

 filter.
 This allow deeper layers = more non-linearity.
\begin_inset Newline newline
\end_inset

(Each stack of 
\begin_inset Formula $3\times3$
\end_inset

 filter enlarge the receptive field of the neuron by 
\begin_inset Formula $1$
\end_inset

 to both sides, so after 
\begin_inset Formula $3$
\end_inset

 stacks you get 
\begin_inset Formula $3\times3\rightarrow5\times5\rightarrow7\times7$
\end_inset

)
\end_layout

\begin_layout Enumerate

\series bold
less parameters
\series default

\begin_inset Newline newline
\end_inset

(Each 
\begin_inset Formula $3\times3$
\end_inset

 uses 
\begin_inset Formula $3\cdot3\cdot C$
\end_inset

 parameters, so 
\begin_inset Formula $3\cdot\left(3^{2}\cdot C\right)=27\cdot C$
\end_inset

, on the other hand 
\begin_inset Formula $7\cdot7\cdot C=49\cdot C$
\end_inset

)
\end_layout

\begin_layout Subsection*
What is the 
\begin_inset Quotes eld
\end_inset

Network within a network
\begin_inset Quotes erd
\end_inset

 feature of the inception model?
\end_layout

\begin_layout Standard
Design a good network topology and stack those onto each others.
\end_layout

\begin_layout Subsection*
What is the structure of the inception model block?
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename inception_model.png
	width 5cm

\end_inset


\end_layout

\begin_layout Subsection*
What are 
\begin_inset Formula $1\times1$
\end_inset

 Conv filter used for?
\end_layout

\begin_layout Standard
To reduce the number of channels without much parameters and without losing
 resolution
\end_layout

\begin_layout Subsection*
What do you add additinal gradient at lower layers?
\end_layout

\begin_layout Standard
You can add auxilary outputs that try to classify the images at earlier
 stages.
\end_layout

\begin_layout Subsection*
When stacking convolutional layers onto each other, what happend to the
 losses as the depth get larger?
\end_layout

\begin_layout Standard
You get higher training and test error 
\begin_inset Formula $\rightarrow$
\end_inset

You don't succeed in over-fitting, even thougth the model is more complex
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename stack_convolution_layer.png
	width 4cm

\end_inset


\end_layout

\begin_layout Subsection*
Describe the ResNet blocks:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename ResNet.png
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
Essentialy 
\begin_inset Formula $F(X)+X$
\end_inset


\end_layout

\begin_layout Subsection*
What does the ResNet block try to solve?
\end_layout

\begin_layout Standard
When building deeper network the input get distorted after many layers,
 and the gradient vanish when going all the way back to the first layers.
 The addition of the input at each block keep the data from distorting too
 much, and allow the gradient to flow more easily back to the first layers.
\end_layout

\begin_layout Subsection*
State 1 technique that allow you to reduce computation cost in ResNet:
\end_layout

\begin_layout Standard
Use 
\begin_inset Quotes eld
\end_inset

bottleneck
\begin_inset Quotes erd
\end_inset

 layers:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename bottleneck_layers.png
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
This avoid doing filters when both the input number of chanels and the output
 number of chanels is high, instead it reduce the dimention of the data
 first to 64 and then increase it back to 256
\end_layout

\begin_layout Subsection*
State 4 ways to improve ResNet?
\end_layout

\begin_layout Enumerate
Wide ResNet (More channels)
\end_layout

\begin_layout Enumerate
Argregated ResNet (each block contain mutiple version that are agragated
 at the end)
\end_layout

\begin_layout Enumerate
Regularization: Stochastic Depth
\end_layout

\begin_layout Enumerate
Squeeze-and-Excitation networks (SENet) - Before adding back the 
\begin_inset Formula $X$
\end_inset

, learn how much to 
\begin_inset Quotes eld
\end_inset

scale
\begin_inset Quotes erd
\end_inset

 each channel
\end_layout

\begin_layout Subsubsection*
How does a Dense Block (from dense net) look like?
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/denseblock.png
	width 3cm

\end_inset


\end_layout

\begin_layout Section*
Week 9 - Recurrent Neural Nets
\end_layout

\begin_layout Subsubsection*
What is the Recurrent Neural Net function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
h_{t} & =\text{State at time }t\\
x_{t} & =\text{Input for time }t\\
F_{W}\left(h,x\right) & =\text{Function with parameters }W\text{ that recive state and input}
\end{align*}

\end_inset


\begin_inset Formula 
\[
h_{t}=F_{W}\left(h_{t-1},x_{t}\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Name 3 RNN structures 'types':
\end_layout

\begin_layout Enumerate
Many to One
\end_layout

\begin_layout Enumerate
Many to Many
\end_layout

\begin_layout Enumerate
One to Many - 
\begin_inset Quotes eld
\end_inset

generate sequence from single input
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsubsection*
Give an example of 
\begin_inset Quotes eld
\end_inset

Many to One
\begin_inset Quotes erd
\end_inset

 RNN
\end_layout

\begin_layout Standard
Sequence of words -> Sentiment
\end_layout

\begin_layout Subsubsection*
Give an example of 
\begin_inset Quotes eld
\end_inset

One to Many
\begin_inset Quotes erd
\end_inset

 RNN
\end_layout

\begin_layout Standard
Image -> Sequence of words
\end_layout

\begin_layout Subsubsection*
Give an example of 
\begin_inset Quotes eld
\end_inset

Many to Many
\begin_inset Quotes erd
\end_inset

 RNN
\end_layout

\begin_layout Standard
Video -> Classification on frame level
\end_layout

\begin_layout Subsection*
Write down te equation for Vanilla RNN
\end_layout

\begin_layout Standard
\begin_inset Formula $h_{t}=tanh\left(W_{hh}h_{t-1}+W_{xh}x_{t}\right)$
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $y_{t}=W_{hy}h_{t}$
\end_inset


\end_layout

\begin_layout Subsubsection*
How do you train a character level RNN on some language model, and output
 same random text from it?
\end_layout

\begin_layout Standard
Train the model such that at every step it should predict the next character
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/rnn_nlp_charlevel_example.png
	width 5cm

\end_inset


\end_layout

\begin_layout Standard
Feed forward the character it predict to generate:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/rnn_nlp_charlevel_example2.png
	width 5cm

\end_inset


\end_layout

\begin_layout Subsubsection*
What 2 ways are there to backpropogate RNN:
\end_layout

\begin_layout Enumerate

\series bold
Backpropogate through time: 
\series default
Unfold the layer repetition as different layers, backpropogate throuth all
 of them
\end_layout

\begin_layout Enumerate

\series bold
Truncated backpropogation through time: 
\series default
Unfold the layer repetition in chunks and back propogate on only those chunks
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/truncated_bpptt.png
	width 4cm

\end_inset


\end_layout

\begin_layout Subsubsection*
What are 2 problems 
\series bold
that Truncated
\series default
 backpropogate help solve?
\end_layout

\begin_layout Enumerate
Single gradient computation require many interation unfolding
\end_layout

\begin_layout Enumerate
Gradient vanish after many iterations
\end_layout

\begin_layout Subsubsection*
How would you build a neural network that generate text descripion of the
 image?
\end_layout

\begin_layout Standard
Encode image into a latent vector (multiple conv and pooling)
\begin_inset Newline newline
\end_inset

Import the vector as a hidden state layer to RNN and generate words from
 it
\end_layout

\begin_layout Subsubsection*
Describe how would you add an attention model to the previous example
\end_layout

\begin_layout Standard
Encode the image into latent matrix of size 
\begin_inset Formula $v=Locations\times Features$
\end_inset

.
\begin_inset Newline newline
\end_inset

At each RNN step output weights vector 
\begin_inset Formula $a=Location\times1$
\end_inset

 which represent weight per location and feed the RNN with single feature
 vector 
\begin_inset Formula $z=\sum_{i=1}^{Locations}a_{i}v_{i}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/attention_model.png
	width 7cm

\end_inset


\end_layout

\begin_layout Subsubsection*
What is multilayer RNN?
\end_layout

\begin_layout Standard
running the input of each timestamp of a RNN as a sequence that goes through
 another RNN
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/mutilayerRNN.png
	width 4cm

\end_inset


\end_layout

\begin_layout Subsubsection*
Give one way to handle vanilla RNN gradient explosion problem:
\end_layout

\begin_layout Standard
gradient clipping
\end_layout

\begin_layout Subsubsection*
What is the structure of LSTM?
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/LSTM.png
	width 12cm

\end_inset


\end_layout

\begin_layout Subsubsection*
What is the problem LSTM network solves and how does it do that?
\end_layout

\begin_layout Standard
Vanilla RNN gradients "vanish", because of the repetative activation function
 and matrix multipication and finite-precision numbers.
\begin_inset Newline newline
\end_inset

LSTM fix this issue by creating an easier way for the gradient to flow through
 uninterrupted, which allow the gradient an easier way not to 
\begin_inset Quotes eld
\end_inset

vanish
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/kessi/Documents/DCNN-Q-A/LSTM_highway.png
	width 8cm

\end_inset


\end_layout

\begin_layout Section*
Week 11a - Detection and Segmentation
\end_layout

\begin_layout Subsubsection*
Define Semantic Segmentation:
\end_layout

\begin_layout Standard
Tag each pixel with the type of element inside it.
 No differentiation between objects of the same type:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted7.png

\end_inset


\end_layout

\begin_layout Subsubsection*
State _ techniques for semantic segmentation:
\end_layout

\begin_layout Enumerate
Sliding Window
\end_layout

\begin_layout Enumerate
Fully Convolutional
\end_layout

\begin_layout Subsubsection*
What is the problem with the sliding window method?
\end_layout

\begin_layout Standard
Inefficient.
 Not sharing features between intersecting windows.
\end_layout

\begin_layout Subsubsection*
What is the problem with the second method?
\end_layout

\begin_layout Standard
complexity of convolutions at original resolution.
\end_layout

\begin_layout Subsubsection*
Solution?
\end_layout

\begin_layout Standard
Downsampling (pooling, and strided convolutions) and then upsampling (unpooling
 and strided transpose convolution)
\end_layout

\begin_layout Subsubsection*
State 3 methods for un-Pooling:
\end_layout

\begin_layout Enumerate
Nearest Neighbor - if max was a 2, set entire region to 2.
\end_layout

\begin_layout Enumerate
Bed of Nails - if max was a 2, set top left corner to 2, all others to the
 minimal value, say 0.
\end_layout

\begin_layout Enumerate
Smart Bed of Nails - keep track of max locations during pooling and place
 the max value back in that location.
\end_layout

\begin_layout Subsubsection*
Describe the transpose convolution:
\end_layout

\begin_layout Standard
Use the input as a weight applied to the convolution filter, use strides
 to re-build the image.
\end_layout

\begin_layout Subsubsection*
Write down the 1D convolution and transpose convolution.
 Using stride 1 and stride >1:
\end_layout

\begin_layout Standard
Stride 1:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\overrightarrow{x}*\overrightarrow{a} & =X\overrightarrow{a}\\
\left(\begin{array}{ccccc}
x & y & z & 0 & 0\\
0 & x & y & z & 0\\
0 & 0 & x & y & z
\end{array}\right) & \left(\begin{array}{c}
0\\
a\\
b\\
c\\
0
\end{array}\right)=\left(\begin{array}{c}
ay+bz\\
ax+by+cz\\
bx+cy
\end{array}\right)
\end{align*}

\end_inset

Stride 1 - transpose:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\overrightarrow{x}*^{T}\overrightarrow{a} & =X^{T}\overrightarrow{a}\\
\left(\begin{array}{ccc}
x & 0 & 0\\
y & x & 0\\
z & y & x\\
0 & z & y\\
0 & 0 & z
\end{array}\right) & \left(\begin{array}{c}
a\\
b\\
c
\end{array}\right)=\left(\begin{array}{c}
ax\\
ay+bx\\
az+by+cx\\
bz+cy\\
cz
\end{array}\right)
\end{align*}

\end_inset

 Stride = 2:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\overrightarrow{x}*\overrightarrow{a} & =X\overrightarrow{a}\\
\left(\begin{array}{ccccc}
x & y & z & 0 & 0\\
0 & 0 & x & y & z
\end{array}\right) & \left(\begin{array}{c}
0\\
a\\
b\\
c\\
0
\end{array}\right)=\left(\begin{array}{c}
ay+bz\\
bx+cy
\end{array}\right)
\end{align*}

\end_inset

Stride 1 - transpose:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\overrightarrow{x}*^{T}\overrightarrow{a} & =X^{T}\overrightarrow{a}\\
\left(\begin{array}{ccc}
x & 0 & 0\\
y & 0 & 0\\
z & x & 0\\
0 & y & 0\\
0 & z & x
\end{array}\right) & \left(\begin{array}{c}
a\\
b
\end{array}\right)=\left(\begin{array}{c}
ax\\
ay\\
az+bx\\
by\\
bz
\end{array}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection*
Describe the 3D reconstruction setting and architecure.
\end_layout

\begin_layout Standard
From set of images to 3D map of points in space.
\begin_inset Newline newline
\end_inset

2D Downsampling (pooling, and strided convolutions), 3D convolutional LSTM,
 and then 3D upsampling (unpooling and strided transpose convolution).
\end_layout

\begin_layout Subsubsection*
Describe _ methods for 2D object detection:
\end_layout

\begin_layout Enumerate
Classification + Localization: a CNN (pretrained on imagenet usually) FC
 to classification softmax loss + FC to box 
\begin_inset Formula $(x,y,w,h)$
\end_inset

 with L2 loss.
\end_layout

\begin_layout Enumerate
Classification sliding window.
\end_layout

\begin_layout Enumerate
R-CNN
\end_layout

\begin_layout Enumerate
Fast R-CNN
\end_layout

\begin_layout Enumerate
Faster R-CNN
\end_layout

\begin_layout Subsubsection*
State the issue with the Classification + Localization method:
\end_layout

\begin_layout Standard
If we have more than one object in the image, we need a different number
 of outputs.
 BAD
\end_layout

\begin_layout Subsubsection*
State the issue with sliding window:
\end_layout

\begin_layout Standard
complexity.
\end_layout

\begin_layout Subsubsection*
State an optimization for classification window
\end_layout

\begin_layout Standard
Region proposals / Selective search.
\end_layout

\begin_layout Subsubsection*
Explain the R-CNN method:
\end_layout

\begin_layout Standard
Training:
\end_layout

\begin_layout Enumerate
Obtain a feature extractor network, which can also classify images (AlexNet
 + fine-tune for example)
\end_layout

\begin_layout Enumerate
Per image in training dataset:
\end_layout

\begin_deeper
\begin_layout Enumerate
Run ROI algorithm.
 Per ROI 
\begin_inset Formula $\sim2K$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Feedforward warped version through network to obtain features.
 Save in large dataset of 
\begin_inset Formula 
\begin{align*}
x & =\left(\text{feature},\text{RoI BBOX}\right)\\
y & =\left(\text{is object},\text{real BBOX}\right)
\end{align*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Train SVM to identify objects.
\end_layout

\begin_layout Enumerate
Train regression models to warp predicted bbox into true bbox
\end_layout

\begin_layout Standard
Test:
\end_layout

\begin_layout Enumerate
Run image through RoI algorithm
\end_layout

\begin_layout Enumerate
Per RoI feedforward warped version through network.
\end_layout

\begin_layout Enumerate
Predict if object using SVM.
\end_layout

\begin_layout Enumerate
If object, predict which and predict true BBOX using regression models.
\end_layout

\begin_layout Subsubsection*
Main 2 problems with R-CNN:
\end_layout

\begin_layout Standard
Slow in train and test (2K ROIs), many ad-hoc training objectives (SVM,
 regression etc)
\end_layout

\begin_layout Subsubsection*
Explain the Fast-R-CNN method:
\end_layout

\begin_layout Standard
Training:
\end_layout

\begin_layout Enumerate
Obtain a feature extractor network, which can also classify images (AlexNet
 + fine-tune for example)
\end_layout

\begin_layout Enumerate
Per image in training dataset:
\end_layout

\begin_deeper
\begin_layout Enumerate
Run ROI algorithm on image to obtain BBOX.
\end_layout

\begin_layout Enumerate
Feedforward image through first 
\begin_inset Formula $n$
\end_inset

 layers of network.
 Per RoI:
\end_layout

\begin_deeper
\begin_layout Enumerate
Project RoI onto this representation.
\end_layout

\begin_layout Enumerate
Warp RoI - divide into 
\begin_inset Formula $7\times7$
\end_inset

 grid and max pool each part.
\end_layout

\begin_layout Enumerate
Feedforward this warped RoI through FC layers to obtain features.
 Save in large dataset of 
\begin_inset Formula 
\begin{align*}
x & =\left(\text{feature},\text{RoI BBOX}\right)\\
y & =\left(\text{is object},\text{real BBOX}\right)
\end{align*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Backprop through: Train classification w/Softmax + linear regression w/Smooth
 L1 loss
\end_layout

\begin_layout Standard
Test:
\end_layout

\begin_layout Enumerate
Run image through RoI algorithm
\end_layout

\begin_layout Enumerate
Feedforward image through first 
\begin_inset Formula $n$
\end_inset

 layers of network.
 Per RoI:
\end_layout

\begin_deeper
\begin_layout Enumerate
Project RoI onto this representation.
\end_layout

\begin_layout Enumerate
Warp RoI - divide into 
\begin_inset Formula $7\times7$
\end_inset

 grid and max pool each part.
\end_layout

\begin_layout Enumerate
Feedforward this warped RoI through FC layers to obtain features.
 Per tuple of 
\begin_inset Formula 
\begin{align*}
x & =\left(\text{feature},\text{RoI BBOX}\right)\\
y & =\left(\text{is object},\text{real BBOX}\right)
\end{align*}

\end_inset

 Predict true BBOX + presence of object.
\end_layout

\end_deeper
\begin_layout Subsubsection*
Write down the smooth 
\begin_inset Formula $L_{1}$
\end_inset

 loss function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
0.5x^{2} & |x|<1\\
|x|-0.5 & \text{else}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
What is the difference between the Fast R-CNN and the Faster R-CNN?
\end_layout

\begin_layout Standard
Faster has an additional Region Proposal Network before the RoI pooling
 (two more losses - classification obj/not obj + bbox smooth L1, in total
 4).
\end_layout

\begin_layout Subsubsection*
State a method for instance segmentation:
\end_layout

\begin_layout Standard
Mask R-CNN.
\end_layout

\begin_layout Subsubsection*
What is YOLO/SSD?
\end_layout

\begin_layout Standard
You only look once, single shot multibox detection.
\end_layout

\begin_layout Subsubsection*
How does the method for YOLO/SSD work?
\end_layout

\begin_layout Standard
Divide image into a 
\begin_inset Formula $7\times7$
\end_inset

 grid.
 For each square: give a score per class of the 
\begin_inset Formula $C$
\end_inset

 classes and regress from each of 
\begin_inset Formula $B$
\end_inset

 base boxes - to final box: 
\begin_inset Formula $(x,y,w,h,\text{confidence})$
\end_inset


\end_layout

\begin_layout Subsubsection*
You need to design an object detection network, speed is crucial accuracy
 not so much.
 Which architecture?
\end_layout

\begin_layout Standard
SSD
\end_layout

\begin_layout Subsubsection*
You need to design an object detection network, accuracy is crucial speed
 not so much.
 Which architecture?
\end_layout

\begin_layout Standard
Faster R-CNN
\end_layout

\begin_layout Subsubsection*
Describe the 3D camera model, give names.
 Specifically, the wierd one!
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted8.png

\end_inset


\end_layout

\begin_layout Subsubsection*
What is the target of a 3D object detection task?
\end_layout

\begin_layout Standard
\begin_inset Formula $\left(x,y,z,w,h,l,r,p,y\right)$
\end_inset

 (roll, pitch yaw)
\end_layout

\begin_layout Subsubsection*
What is the simplified 3D bbox setting?
\end_layout

\begin_layout Standard
No roll and pitch.
\end_layout

\begin_layout Subsubsection*
What is the general method for 3D object detection?
\end_layout

\begin_layout Standard
Similar structure to Faster R-CNN.
 combine 3D proposals from many views and sensors.
 regress bbox numbers.
\end_layout

\begin_layout Section*
Week 11 - generative models
\end_layout

\begin_layout Subsubsection*
What are the two types of density estimation?
\end_layout

\begin_layout Enumerate
explicit - explicitly solve for and define 
\begin_inset Formula $p_{\text{model}}\left(x\right)\sim p_{\text{data}}\left(x\right)$
\end_inset


\end_layout

\begin_layout Enumerate
implicit - can only sample from some 
\begin_inset Formula $p_{\text{model}}\left(x\right)\sim p_{\text{data}}\left(x\right)$
\end_inset


\end_layout

\begin_layout Subsubsection*
Draw the taxonomy graph of generative models
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted9.png
	width 13cm

\end_inset


\end_layout

\begin_layout Subsubsection*
Describe the main concept behind Fully Visible Belief Networks:
\end_layout

\begin_layout Standard
Generate an entire picture via 1D distributions:
\begin_inset Formula 
\[
\prod_{i=1}^{n}p\left(x_{i}|x_{1},....,x_{i-1}\right)
\]

\end_inset

Each 
\begin_inset Formula $p$
\end_inset

 is expressed via NN.
 Optimization via Maximum Likelihood estimation on training data.
 Softmax loss on distribution outputed by NN.
\end_layout

\begin_layout Subsubsection*
Describe the PixelRNN
\end_layout

\begin_layout Standard
Generate image starting from corner.
 Spatial LSTM.
\end_layout

\begin_layout Subsubsection*
DrawBack?
\end_layout

\begin_layout Standard
Slow.
 Must work sequentially in training.
\end_layout

\begin_layout Subsubsection*
Describe the PixelCNN
\end_layout

\begin_layout Standard
Row by row, pixel by pixel.
 Generate via CNN over context region.
\end_layout

\begin_layout Subsubsection*
Which is faster? State 2 reasons.
\end_layout

\begin_layout Standard
PixelCNN:
\end_layout

\begin_layout Enumerate
Bound yourself to a smaller context region (no entire top-left hand part
 of image).
\end_layout

\begin_layout Enumerate
Can parralelize training (but not generation)! In the LSTM version we get
 a hidden state based on all previous images.
 In the PixelCNN version we can choose any pixel and use the context from
 it's surrounding in the training image.
\end_layout

\begin_layout Subsubsection*
State 3 Pros of these two models:
\end_layout

\begin_layout Enumerate
Explicitly compute 
\begin_inset Formula $p\left(x\right)$
\end_inset

 (how exactly?)
\end_layout

\begin_layout Enumerate
Empirical likelihood of training data can serve as validation
\end_layout

\begin_layout Enumerate
Good samples (huh?)
\end_layout

\begin_layout Subsubsection*
State a con of these methods
\end_layout

\begin_layout Standard
Sequential generation of pixels is slow.
\end_layout

\begin_layout Subsubsection*
Describe how to use an Autoencoder for a supervised learning task with few
 samples:
\end_layout

\begin_layout Enumerate
Train the autoencoder.
\end_layout

\begin_layout Enumerate
Drop the decoder
\end_layout

\begin_layout Enumerate
Fine tune the encoder + FC classifier part on the few labelled examples.
\end_layout

\begin_layout Subsubsection*
Describe how to use an autoencoder for generating data:
\end_layout

\begin_layout Standard
We assume that data is generated in the following manner:
\end_layout

\begin_layout Standard
Sample 
\begin_inset Formula $z$
\end_inset

 from true prior
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p_{\theta^{*}}\left(z\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Sample 
\begin_inset Formula $x$
\end_inset

 from true conditional:
\begin_inset Formula 
\[
p_{\theta^{*}}\left(x|z\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
How do we usually represent 
\begin_inset Formula $p_{\theta^{*}}\left(z\right)$
\end_inset

, and why?
\end_layout

\begin_layout Standard
A simple distribution, say Gaussian.
 It is reasonable to assume that the latent variables are distributed such
 (cat images - size of cat in image, pose, head rotation etc.).
\end_layout

\begin_layout Subsubsection*
How can we make independent features?
\end_layout

\begin_layout Standard
Diagonal 
\begin_inset Formula $\Sigma$
\end_inset


\end_layout

\begin_layout Subsubsection*
State the function for expressing likelihood of sample 
\begin_inset Formula $x$
\end_inset

, in the latent variable setting:
\end_layout

\begin_layout Standard
\begin_inset Formula $p_{\theta}\left(x\right)=\int_{z}p_{\theta}\left(z\right)\cdot p_{\theta}\left(x|z\right)dz$
\end_inset


\end_layout

\begin_layout Subsubsection*
What is the problem with trying to compute 
\begin_inset Formula $\theta$
\end_inset

 which maximizes likelihood in this setting?
\end_layout

\begin_layout Standard
Intractable, no way we can optimize for all 
\begin_inset Formula $z$
\end_inset


\end_layout

\begin_layout Subsubsection*
The solution involves using an encoder and decoder network, state what they
 compute:
\end_layout

\begin_layout Standard
Encoder network computes traits of the distribution of 
\begin_inset Formula $z$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

.
 Density function expressed as:
\begin_inset Formula $q_{\phi}\left(z|x\right)$
\end_inset

.
 Samples:
\begin_inset Formula 
\[
z|x\sim\mathcal{N}\left(\mu_{z|x},\Sigma_{z|x}\right)
\]

\end_inset

Decoder network computes traits of the distribution of 
\begin_inset Formula $x$
\end_inset

 given 
\begin_inset Formula $z$
\end_inset

.
 Density function expressed as:
\begin_inset Formula $p_{\theta}\left(x|z\right)$
\end_inset

.
 Samples:
\begin_inset Formula 
\[
x|z\sim\mathcal{N}\left(\mu_{x|z},\Sigma_{x|z}\right)
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
What is ELBO?
\end_layout

\begin_layout Standard
Evidence lower bound.
 It is a lower bound on the likelihood of our training data given paramaters
 
\begin_inset Formula $\theta,\phi$
\end_inset

 of our network.
 We optimize 
\begin_inset Formula $\theta,\phi$
\end_inset

 such that this lower bound is maximized.
\end_layout

\begin_layout Subsubsection*
State the KL divergence of two distributions 
\begin_inset Formula $P,Q$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
D_{KL}\left(P||Q\right) & =\Sigma_{x\in X}P\left(x\right)\log\left(\frac{P\left(x\right)}{Q(x)}\right)=-\Sigma_{x\in X}P\left(x\right)\log\left(\frac{Q\left(x\right)}{P(x)}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection*
Develop the ELBO expression:
\end_layout

\begin_layout Standard
For some sample 
\begin_inset Formula $x$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\log\left(p_{\theta}\left(x\right)\right) & =\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(p_{\theta}\left(x\right)\right)\right]
\end{align*}

\end_inset

 True for any pair of random variables which do not depend on eachother.
 Now we use bayes' to express 
\begin_inset Formula $p_{\theta}\left(x\right)$
\end_inset

 differently:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
p_{\theta}\left(x\right) & =\frac{p_{\theta}\left(x\wedge z\right)}{\frac{p_{\theta}\left(x\wedge z\right)}{p_{\theta}\left(x\right)}}\\
 & =\frac{p_{\theta}\left(z\right)\cdot p_{\theta}\left(x|z\right)}{p_{\theta}\left(z|x\right)}
\end{align*}

\end_inset

 Let's resume developing the lower bound:
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(p_{\theta}\left(x\right)\right)\right] & =\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(\frac{p_{\theta}\left(z\right)\cdot p_{\theta}\left(x|z\right)}{p_{\theta}\left(z|x\right)}\right)\right]\\
 & =\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(\frac{p_{\theta}\left(z\right)\cdot p_{\theta}\left(x|z\right)}{p_{\theta}\left(z|x\right)}\frac{q_{\phi}\left(z|x\right)}{q_{\phi}\left(z|x\right)}\right)\right]\\
 & =\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(p_{\theta}\left(x|z\right)\right)-\log\left(\frac{q_{\phi}\left(z|x\right)}{p_{\theta}\left(z\right)}\right)+\log\left(\frac{q_{\phi}\left(z|x\right)}{p_{\theta}\left(z|x\right)}\right)\right]\\
 & =\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(p_{\theta}\left(x|z\right)\right)\right]-\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(\frac{q_{\phi}\left(z|x\right)}{p_{\theta}\left(z\right)}\right)\right]+\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(\frac{q_{\phi}\left(z|x\right)}{p_{\theta}\left(z|x\right)}\right)\right]\\
 & =\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(p_{\theta}\left(x|z\right)\right)\right]-D_{KL}\left(q_{\phi}\left(z|x\right)||p_{\theta}\left(z\right)\right)+D_{KL}\left(q_{\phi}\left(z|x\right)||p_{\theta}\left(z|x\right)\right)\\
 & \geq\mathbb{E}_{z\sim q_{\phi}\left(z|x\right)}\left[\log\left(p_{\theta}\left(x|z\right)\right)\right]-D_{KL}\left(q_{\phi}\left(z|x\right)||p_{\theta}\left(z\right)\right)\\
 & :=\mathcal{L}\left(x,\theta,\phi\right)
\end{align*}

\end_inset

Last inequality because 
\begin_inset Formula $D_{KL}\geq0$
\end_inset

 always.
\end_layout

\begin_layout Subsubsection*
State the optimization objective of training a VAE using the above:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\theta^{*},\phi^{*} & =\arg\underset{\theta,\phi}{\max}\sum_{i=1}^{N}\mathcal{L}\left(x^{\left(i\right)},\theta,\phi\right)
\end{align*}

\end_inset

 This term is differentiable!
\end_layout

\begin_layout Subsubsection*
State 2 pros and 2 cons of VAEs:
\end_layout

\begin_layout Standard
Pros:
\end_layout

\begin_layout Enumerate
Principled approach to generation
\end_layout

\begin_layout Enumerate
Get a feature extractor on the way
\end_layout

\begin_layout Standard
Cons:
\end_layout

\begin_layout Enumerate
Maximize a lower bound..
\end_layout

\begin_layout Enumerate
Generally blurrier and lower quality images than GANs, which are state of
 the art.
\end_layout

\begin_layout Subsubsection*
Why are they called Variational Autoencoders?
\end_layout

\begin_layout Standard
Still don't know.
\end_layout

\end_body
\end_document
